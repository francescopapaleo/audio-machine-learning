{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDilationCNN(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super(ExponentialDilationCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i\n",
    "            self.layers.append(nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, dilation=dilation))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_receptive_field(self):\n",
    "        receptive_field = 1\n",
    "        for layer in self.layers:\n",
    "            dilation = layer.dilation[0]\n",
    "            kernel_size = layer.kernel_size[0]\n",
    "            receptive_field += (kernel_size - 1) * dilation\n",
    "        return receptive_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDilationCNN(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super(LinearDilationCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = i + 1\n",
    "            self.layers.append(nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, dilation=dilation))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def calc_receptive_field(self):\n",
    "        receptive_field = 1\n",
    "        for layer in self.layers:\n",
    "            dilation = layer.dilation[0]\n",
    "            kernel_size = layer.kernel_size[0]\n",
    "            receptive_field += (kernel_size - 1) * dilation\n",
    "        return receptive_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp = ExponentialDilationCNN(num_layers=4)\n",
    "    model_lin = LinearDilationCNN(num_layers=4)\n",
    "\n",
    "    dummy_input = torch.randn(1, 1, 512)\n",
    "\n",
    "    summary_exp = summary(model_exp, input_size=dummy_input.shape, depth=4, verbose=1)\n",
    "    summary_lin = summary(model_lin, input_size=dummy_input.shape, depth=4, verbose=1)\n",
    "\n",
    "    print(summary_exp)\n",
    "    exp_receptive_field = model_exp.calc_receptive_field()\n",
    "    print(f\"Exponential Model Receptive Field: {exp_receptive_field}\\n\\n\")\n",
    "\n",
    "    print(summary_lin)\n",
    "    lin_receptive_field = model_lin.calc_receptive_field()\n",
    "    print(f\"Linear Model Receptive Field: {lin_receptive_field}\\n\\n\")\n",
    "\n",
    "    for i in range(4):\n",
    "        print(f\"Exponential Model Layer {i + 1} - Dilation: {2**i}, Receptive Field: {model_exp.layers[i].dilation[0]}\\n\")\n",
    "\n",
    "    for i in range(4):\n",
    "        print(f\"Linear Model Layer {i + 1} - Dilation: {i + 1}, Receptive Field: {model_lin.layers[i].dilation[0]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
